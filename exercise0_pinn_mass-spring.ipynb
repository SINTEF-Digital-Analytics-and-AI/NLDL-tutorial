{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN for a damped mass-spring system\n",
    "\n",
    "This notebook, mainly written by Ben Tapley and inspired by [this repo](https://github.com/benmoseley/harmonic-oscillator-pinn), uses [physics informed neural networks (PINNs)](https://doi.org/10.1016/j.jcp.2018.10.045) to model a damped mass-spring system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises:\n",
    "\n",
    "* Run the standard neural network model to learn the dynamics. Experiment with the hyperparameters and see if you can get the model to generalise better outside the domain.\n",
    "* Train a PINN model on the same data and plot the results. Experiment with hyperparameters and data parameters.\n",
    "* Train a physics informed neural network but learn the parameters $\\mu$ and $k$ (i.e., treat them as unknown parameters). \n",
    "* Add noise and repeat the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from tqdm import trange\n",
    "    import seaborn as sns\n",
    "except ModuleNotFoundError:\n",
    "    import os\n",
    "    if not os.path.exists(\"requirements.txt\"):\n",
    "        print(\"Downloading requirements.txt from GitHub...\")\n",
    "        import urllib.request\n",
    "        url = \"https://raw.githubusercontent.com/SINTEF-Digital-Analytics-and-AI/NLDL-tutorial/main/requirements.txt\"\n",
    "        urllib.request.urlretrieve(url, \"requirements.txt\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from tqdm import trange\n",
    "    import seaborn as sns\n",
    "if int(np.__version__.split('.')[0]) >= 2:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    print(\"NumPy version >= 2 detected. Downgrading to a compatible version...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy<2\"])\n",
    "    print(\"Please restart the kernel and rerun the script.\")\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.random.manual_seed(1)\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['lines.markersize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "colors = sns.color_palette([(0.6,0.8,.8), (1,0.7,0.3), (0.2,0.7,0.2), (0.8,0,0.2), (0,0.4,1), (0.6,0.5,.9), (0.5,0.3,.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact solution and data\n",
    "Here, we will learn the dynamics of a 1D damped mass-spring system with unit mass, spring constant $k$ and damping coefficient $\\mu$:\n",
    "$$\n",
    "\\dfrac{d^2 x}{d t^2} + \\mu \\dfrac{d x}{d t} + kx = 0~.\n",
    "$$\n",
    "\n",
    "For the underdamped case, $\\mu^2 < 4 k~,$ the exact solution is given by \n",
    "$$\n",
    "x(t) = e^{-\\delta t}(A \\cos(\\phi + \\omega t)),\n",
    "$$\n",
    "where $\\omega=\\frac{1}{2}\\sqrt{4k - \\mu^2}$ and the constants $A$ and $\\phi$ determine the initial conditions. We will set $A=1$ and $\\phi=0$.\n",
    "\n",
    "Let us turn the above exact solution into a function that we can use to generate a training data set $X_{\\mathrm{train}}$ consisting of points $(t_i, x_i)$, where $t_i$ are evenly spaced times on some interval and $x_i=x(t_i)$ is given by the above exact solution. We will also create a test data set $X_{\\mathrm{test}}$ on a longer time interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ODE parameters\n",
    "MU = 0.4\n",
    "K = 4\n",
    "\n",
    "# Data generation parameters\n",
    "N_TRAIN = 20\n",
    "TMAX_TRAIN = 2\n",
    "NOISE_STD = 0.\n",
    "\n",
    "N_TEST = 50\n",
    "TMAX_TEST = 10\n",
    "_\n",
    "def exact_solution(t, k=K, mu=MU):\n",
    "    \"\"\"Get exact solution to the 1D underdamped harmonic oscillator.\"\"\"\n",
    "    assert mu**2 < 4 * k, \"System must be underdamped.\"\n",
    "    w = np.sqrt(4 * k - mu**2) / 2\n",
    "    x = torch.exp(-mu / 2 * t) * torch.cos(w * t)\n",
    "    return x\n",
    "\n",
    "\n",
    "t_train = torch.linspace(0, TMAX_TRAIN, N_TRAIN + 1).unsqueeze(-1)\n",
    "t_test = torch.linspace(0, TMAX_TEST, N_TEST + 1).unsqueeze(-1)\n",
    "\n",
    "x_train = exact_solution(t_train) + NOISE_STD * torch.randn_like(t_train)\n",
    "x_test = exact_solution(t_test)\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(t_test, x_test, \"k-\", label=\"Test data (exact sol.)\")\n",
    "plt.plot(t_train, x_train, color = colors[0], linestyle=\"none\", marker=\".\", label=f'Training data')\n",
    "plt.title(\"Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the dynamics with a purely data-driven model\n",
    "Now we will set up a neural network $x_\\theta(t)$ to learn the dynamics $x(t)$ from the training data. We hope that it will generalise well to the test data, which is the exact solution over a longer time span. \n",
    "\n",
    "We do this by training the neural network on a loss function that minimises the mean squared error (MSE) between the predicted solution $x_{\\theta}(t_i)$ and the observed (exact solution) values $(t_i, x_i)\\in X_{\\mathrm{train}}$:\n",
    "\n",
    "$$L_{\\mathrm{data}} = \\sum_{(t_i, x_i)\\in X_{\\mathrm{train}}}\\|x_{\\theta}(t_i) - x_i\\|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=32, output_size=1):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def train_nn(model, t_train, x_train, nepochs=10000, learning_rate=5e-3):\n",
    "    mse = nn.MSELoss()\n",
    "    torch.manual_seed(123)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses = []\n",
    "    \n",
    "    with trange(nepochs, desc=\"Training the model\") as pbar:\n",
    "        for i in range(nepochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute the loss\n",
    "            x_pred = model(t_train)\n",
    "            loss = mse(x_pred, x_train)\n",
    "\n",
    "            # Backward pass and update parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log the loss value\n",
    "            losses.append(loss.item())\n",
    "            if i % 100 == 0 or i == nepochs - 1:\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Plot the loss curve\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(losses)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model_nn = NeuralNet()\n",
    "train_nn(model_nn, t_train=t_train, x_train=x_train)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(t_test, x_test, \"k-\", label=\"Exact solution\")\n",
    "plt.plot(t_train, x_train, color = colors[0], linestyle=\"none\", marker=\".\", label=f'Training data')\n",
    "plt.plot(t_test, model_nn(t_test).detach().numpy(), \"r-\", label=\"Standard NN\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the dynamics with a physics informed neural network\n",
    "Now we will train the physics informed neural network (PINN). This is done in exactly the same way as before, but we augment the loss function with the \"physics loss\". That is,\n",
    "\n",
    "$$L_{\\mathrm{PINN}}=L_{\\mathrm{data}} + \\lambda L_{\\mathrm{phys.}}$$\n",
    "where $\\lambda$ is a hyperparameter, and\n",
    "$$L_{\\mathrm{phys.}} = \\sum_{t_i\\in X_{\\mathrm{phys.}}}\\|\\ddot{x}_{\\theta}(t_i)+ \\mu\\,\\dot{x}_{\\theta}(t_i) + k\\,x_{\\theta}(t_i)\\|^2,$$\n",
    "where $X_{\\mathrm{phys.}}$ are a set of times, chosen by us, that we choose to evaluate the *physics loss* on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose points to evaluate the physics loss with\n",
    "TMAX_PHYS = 7\n",
    "N_PHYS = 200\n",
    "\n",
    "t_phys = np.random.uniform(0, TMAX_PHYS, N_PHYS)\n",
    "\n",
    "model_pinn = NeuralNet(1, 32, 1)\n",
    "\n",
    "def time_derivative(x, t):\n",
    "    \"\"\"Returns the time derivative of x at times t using automatic differentiation.\n",
    "    Example: \n",
    "        t = torch.linspace(0, 1, 10)\n",
    "        x = model(t)\n",
    "        xdot = time_derivative(x, t)\"\"\"\n",
    "    xdot = torch.autograd.grad(x, t, torch.ones_like(x), create_graph=True)[0]\n",
    "    return xdot\n",
    "\n",
    "def train_pinn(model, t_train, x_train, t_phys, nepochs=10000, learning_rate=5e-3):\n",
    "    lambda_ = 1e-1\n",
    "    mse = nn.MSELoss()\n",
    "    t_phys = torch.tensor(t_phys, requires_grad=True, dtype=torch.float32).reshape(-1, 1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses = []\n",
    "\n",
    "    with trange(nepochs, desc=\"Training PINN\") as pbar:\n",
    "        for i in range(nepochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute data loss\n",
    "            x_pred = model(t_train)\n",
    "            loss = mse(x_pred, x_train)\n",
    "\n",
    "            # Compute physics loss\n",
    "            x_phys = model(t_phys)\n",
    "            xdot = time_derivative(x_phys, t_phys)\n",
    "            xddot = time_derivative(xdot, t_phys)\n",
    "            ode_residual = xddot + MU * xdot + K * x_phys\n",
    "            loss += lambda_ * torch.mean(ode_residual**2)\n",
    "\n",
    "            # Backward pass and parameter update\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log loss value\n",
    "            losses.append(loss.item())\n",
    "            if i % 100 == 0 or i == nepochs - 1:\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Plot the loss curve\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(losses)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve for PINN')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Training the PINN\n",
    "train_pinn(model_pinn, t_train, x_train, t_phys)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(t_test, x_test, \"k-\", label=\"Exact solution\")\n",
    "plt.plot(t_train, x_train, color = colors[0], linestyle=\"none\", marker=\".\", label='Training data')\n",
    "plt.plot(t_phys, 0 * t_phys, color = colors[1], linestyle=\"none\", marker=\".\", label=\"Physics loss points\")\n",
    "plt.plot(t_test, model_nn(t_test).detach().numpy(), \"r-\", label=\"Standard NN\")\n",
    "plt.plot(t_test, model_pinn(t_test).detach().numpy(), \"b-\", label=\"PINN\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$x$\")\n",
    "plt.legend(bbox_to_anchor=(0.5, -0.2), loc='upper center', ncol=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the parameters $\\mu$ and $k$\n",
    "\n",
    "What we have just done was solve the forward problem. Traditional numerics usually beats PINNs in terms of cost and accuracy. However, the scenario where PINNs outperforms traditional numerics is when we have *partial* knowledge of the governing equations. \n",
    "\n",
    "Say that the parameters $\\mu$ and $k$ are unknown. Using the data (which encodes the values of $\\mu$ and $k$) together with knowledge that the governing ODE is partially known, we can treat $\\mu$ and $k$ as learnable parameters and adopt the same approach as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=1, hidden_size=32, output_size=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Tell the pytorch module to learn the parameters:\n",
    "        self.mu = nn.Parameter(torch.tensor(0.0, requires_grad=True))\n",
    "        self.k = nn.Parameter(torch.tensor(0.0, requires_grad=True))\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_pinn(model, t_train, x_train, t_phys, nepochs=10000, learning_rate=5e-3):\n",
    "    lambda_ = 1e-1\n",
    "    mse = nn.MSELoss()\n",
    "    t_phys = torch.tensor(t_phys, requires_grad=True, dtype=torch.float32).reshape(-1, 1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses = []\n",
    "\n",
    "    with trange(nepochs, desc=\"Training PINN with parameters\") as pbar:\n",
    "        for i in range(nepochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute data loss\n",
    "            x_pred = model(t_train)\n",
    "            loss = mse(x_pred, x_train)\n",
    "\n",
    "            # Compute physics loss (parameters mu and k in the ODE)\n",
    "            mu = model.mu\n",
    "            k = model.k\n",
    "            x_phys = model(t_phys)\n",
    "            xdot = time_derivative(x_phys, t_phys)\n",
    "            xddot = time_derivative(xdot, t_phys)\n",
    "            ode_residual = xddot + mu * xdot + k * x_phys\n",
    "            loss += lambda_ * torch.mean(ode_residual**2)\n",
    "\n",
    "            # Backward pass and parameter update\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log loss value\n",
    "            losses.append(loss.item())\n",
    "            if i % 100 == 0 or i == nepochs - 1:\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Plot the loss curve\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(losses)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve for PINN with Parameters')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define the model and train\n",
    "model_pinn2 = PINN(1, 32, 1)\n",
    "train_pinn(model_pinn2, t_train, x_train, t_phys)\n",
    "\n",
    "print(\n",
    "    f\"learned μ = {model_pinn2.mu.item():.3f}, exact μ = {MU}\\nlearned k = {model_pinn2.k.item():.3f}, exact k = {K}\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(t_test, x_test, \"k-\", label=\"Exact solution\")\n",
    "plt.plot(t_train, x_train, color = colors[0], linestyle=\"none\", marker=\".\", label='Training data')\n",
    "plt.plot(t_phys, 0 * t_phys, color = colors[1], linestyle=\"none\", marker=\".\", label=\"Physics loss points\")\n",
    "plt.plot(t_test, model_nn(t_test).detach().numpy(), \"r-\", label=\"Standard NN\")\n",
    "plt.plot(t_test, model_pinn2(t_test).detach().numpy(), \"b-\", label=\"PINN\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$x$\")\n",
    "plt.legend(bbox_to_anchor=(0.5, -0.2), loc='upper center', ncol=5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nldl_env)",
   "language": "python",
   "name": "nldl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
