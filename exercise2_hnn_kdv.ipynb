{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HNN for the KdV equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a Hamiltonian neural network (HNN) model to learn the Korteweg--de Vries (KdV) equation, given by\n",
    "\\begin{equation}\n",
    "u_t + \\eta u u_x + \\gamma^2 u_{xxx} = 0,\n",
    "\\end{equation}\n",
    "where $\\eta, \\gamma \\in \\mathbb{R}$. The Hamiltonian\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{H}[u] &= \\int_\\mathbb{R} \\left(-\\frac{\\eta}{6} u^3 + \\frac{\\gamma^2}{2}u_x^2 \\right)\\, dx\n",
    "\\end{align*}\n",
    "$$\n",
    "represents the energy, and is conserved, i.e. constant over time.\n",
    "\n",
    "The variational derivative of the Hamiltonian is\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\delta\\mathcal{H}}{\\delta u}[u] &= - \\frac{\\eta}{2} u^2 - \\gamma^2 u_{xx},\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "and the KdV equation may be expressed by a product of the skew-symmetric operator $\\frac{\\partial}{\\partial x}$ and this variational derivative. That is,\n",
    "\\begin{equation*}\n",
    "u_t = - \\frac{\\partial}{\\partial x} \\left( \\frac{\\eta}{2} u^2 + \\gamma^2 u_{xx} \\right),\n",
    "\\end{equation*}\n",
    "which we see is equivalent to (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises:\n",
    "* Change `N_TRAIN`, `TRAIN_TSTEP` and `TMAX_TRAIN` to experiment with different numbers of training data points and the end point of each evolution\n",
    "* Finish setting up the HNN by specifying the kernel size of the first convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from tqdm import trange\n",
    "    import seaborn as sns\n",
    "    import phlearn.phsystems.pde as phsys\n",
    "    import phlearn.phnns as phnn\n",
    "    from scipy.sparse import spdiags\n",
    "except ModuleNotFoundError:\n",
    "    import os\n",
    "    if not os.path.exists(\"requirements.txt\"):\n",
    "        print(\"Downloading requirements.txt from GitHub...\")\n",
    "        import urllib.request\n",
    "        url = \"https://raw.githubusercontent.com/SINTEF-Digital-Analytics-and-AI/NLDL-tutorial/main/requirements.txt\"\n",
    "        urllib.request.urlretrieve(url, \"requirements.txt\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from tqdm import trange\n",
    "    import seaborn as sns\n",
    "    import phlearn.phsystems.pde as phsys\n",
    "    import phlearn.phnns as phnn\n",
    "    from scipy.sparse import spdiags\n",
    "if int(np.__version__.split('.')[0]) >= 2:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    print(\"NumPy version >= 2 detected. Downgrading to a compatible version...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy<2\"])\n",
    "    print(\"Please restart the kernel and rerun the script.\")\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.random.manual_seed(1)\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['lines.markersize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "colors = sns.color_palette([(0.6,0.8,.8), (1,0.7,0.3), (0.2,0.7,0.2), (0.8,0,0.2), (0,0.4,1), (0.6,0.5,.9), (0.5,0.3,.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data\n",
    "\n",
    "We use the [phlearn](https://github.com/SINTEF/pseudo-hamiltonian-neural-networks) package to set up the system and generate training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 6.\n",
    "gamma = 1.\n",
    "period = 20\n",
    "spatial_points = 100\n",
    "x = np.linspace(0, period-period/spatial_points, spatial_points)\n",
    "\n",
    "def setup_KdV_system(x=x, eta=6., gamma=1.):\n",
    "    \n",
    "    M = x.size\n",
    "    dx = x[-1]/(M-1)\n",
    "    e = np.ones(M)\n",
    "    Dp = 1/dx*spdiags([e,-e,e], np.array([-M+1,0,1]), M, M).toarray() # Forward difference matrix\n",
    "    D1 = .5/dx*spdiags([e,-e,e,-e], np.array([-M+1,-1,1,M-1]), M, M).toarray() # Central difference matrix\n",
    "    D2 = 1/dx**2*spdiags([e,e,-2*e,e,e], np.array([-M+1,-1,0,1,M-1]), M, M).toarray() # 2nd order central difference matrix\n",
    "\n",
    "    def hamiltonian(u):\n",
    "        return np.sum(-1/6*eta*u**3 + (.5*gamma**2*(np.matmul(Dp,u.T))**2).T, axis=-1)\n",
    "\n",
    "    def hamiltonian_grad(u):\n",
    "        return -.5*eta*u**2 - (gamma**2 * u @ D2)\n",
    "    \n",
    "    def initial_condition():\n",
    "        P = (x[-1]-x[0])*M/(M-1)\n",
    "        sech = lambda a: 1/np.cosh(a)\n",
    "        def sampler(rng):\n",
    "            k1, k2 = rng.uniform(0.5, 1., 2)\n",
    "            d1, d2 = rng.uniform(0., 1., 1), rng.uniform(0., 1., 1)\n",
    "            u0 = 0\n",
    "            u0 += (-6./-eta)*2 * k1**2 * sech(np.abs(k1 * ((x+P/2-P*d1) % P - P/2)))**2\n",
    "            u0 += (-6./-eta)*2 * k2**2 * sech(np.abs(k2 * ((x+P/2-P*d2) % P - P/2)))**2\n",
    "            u0 = np.concatenate([u0[M:], u0[:M]], axis=-1)\n",
    "            return u0\n",
    "        return sampler\n",
    "\n",
    "    KdV_system = phsys.PseudoHamiltonianPDESystem(\n",
    "        nstates=M,\n",
    "        skewsymmetric_matrix=D1,\n",
    "        hamiltonian=hamiltonian,\n",
    "        grad_hamiltonian=hamiltonian_grad,\n",
    "        init_sampler=initial_condition()\n",
    "    )\n",
    "\n",
    "    return KdV_system\n",
    "\n",
    "\n",
    "KdV_system = setup_KdV_system(eta=eta, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(system, data_points=5, dt=.02, tmax=.02, x=x):\n",
    "    nt = round(tmax / dt)\n",
    "    t_axis = np.linspace(0, tmax, nt + 1)\n",
    "    ntrajectories_train = int(np.ceil(data_points / nt))\n",
    "    traindata = phnn.generate_dataset(system, ntrajectories_train, t_axis, xspatial=x)\n",
    "    return traindata, t_axis, ntrajectories_train, nt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Experiment with different number of training data points and the end point of each evolution, which determines how many different evolutions you have (by `ntrajectories_train = int(np.ceil(data_points / nt))`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN = 1 # Number of training states (one data point is a solution at every spatial step at one time)\n",
    "TRAIN_TSTEP = 0.02 # Time step\n",
    "TMAX_TRAIN = 0.02 # End time of each evolution in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata, t_axis, ntrajectories_train, nt = get_training_data(KdV_system, data_points=N_TRAIN, dt=TRAIN_TSTEP, tmax=TMAX_TRAIN)\n",
    "\n",
    "# Extracting the necessary data and reshaping the arrays:\n",
    "u_start = traindata[0][0]\n",
    "u_end = traindata[0][1]\n",
    "u_midpoint = (u_start+u_end)/2\n",
    "dudt = traindata[1]\n",
    "x = x.reshape((1,-1))\n",
    "t = t_axis.reshape((1,-1))[:,:-1]\n",
    "u_exact = u_start.squeeze(1).detach().numpy()\n",
    "dx = (x[..., 1] - x[..., 0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_fractions = [1/4, 1/2, 3/4, 1-1/nt]\n",
    "max_plots = min(nt, len(time_fractions))\n",
    "\n",
    "for fraction in time_fractions[:max_plots]:\n",
    "    fig = plt.figure(figsize=(10, 3))\n",
    "    i_time = int(round(nt * fraction))\n",
    "    for i in range(min(ntrajectories_train, 5)):\n",
    "        plt.plot(x[0, :], u_exact[i_time + i * nt, :], linewidth=2, label=f'Evolution {i+1}')\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$u(t,x)$')\n",
    "    plt.title(f'$t = {t[0, i_time]:.2f}$')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the HNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We impose the periodic boundary conditions by padding the solution vectors.\n",
    "\n",
    "**Exercise:** Set the size of the first convolutional kernel so that we can learn finite difference operators of the order necessary to approximate the spatial derivatives we wish to model. Note that it should correspond to the padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HNN(nn.Module):\n",
    "    def __init__(self, conv_kernel_size=SET_KERNEL_SIZE):\n",
    "        super().__init__()\n",
    "        self.padding_size = 1\n",
    "        self.hamiltonian_net = nn.Sequential(\n",
    "            nn.Conv1d(1, 100, kernel_size=conv_kernel_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(100, 100, kernel_size=1),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(100, 100, kernel_size=1, bias=None)\n",
    "        )\n",
    "\n",
    "    def forward_padding(self, x):\n",
    "        return torch.cat([x, x[..., :self.padding_size]], dim=-1)\n",
    "\n",
    "    def summation(self, x):\n",
    "        return x.sum(dim=tuple(range(1, x.ndim)), keepdim=True)\n",
    "\n",
    "    def hamiltonian(self, u):\n",
    "        u_padded = self.forward_padding(u)\n",
    "        H = self.hamiltonian_net(u_padded)\n",
    "        return self.summation(H)\n",
    "\n",
    "    def forward(self, u, dx):\n",
    "        H = self.hamiltonian(u)\n",
    "        dH = torch.autograd.grad(H.sum(), u, create_graph=True)[0]\n",
    "        dH_padded = torch.cat([dH[..., u.shape[-1] - 1 :], dH, dH[..., :1]], dim=-1)\n",
    "        S = (torch.tensor([-1., 0., 1.], dtype=torch.float32) / (2 * dx)).reshape(1, 1, 3).to(u.device)\n",
    "        return torch.nn.functional.conv1d(dH_padded, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, dxdt, nepochs=10000, learning_rate=1e-3, **kwargs):\n",
    "\n",
    "    # Move model to device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    # Convert inputs to tensors and move to device\n",
    "    x = torch.tensor(x, requires_grad=True, dtype=torch.float32).to(device)\n",
    "    dxdt = torch.tensor(dxdt, dtype=torch.float32).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses = []\n",
    "    \n",
    "    with trange(nepochs, desc=\"Training the model\") as pbar:\n",
    "        for epoch in range(nepochs):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute loss by comparing the left-hand side (LHS) and the right-hand side (RHS) of the discretized PDE:\n",
    "            rhs = model(x, **kwargs)\n",
    "            loss = torch.mean((dxdt - rhs) ** 2)\n",
    "            \n",
    "            # Backpropagation and optimization step:\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Log the loss value:\n",
    "            losses.append(loss.item())\n",
    "            if epoch % 100 == 0 or epoch == nepochs - 1:\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Plot the loss curve\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(losses)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HNN()\n",
    "train(model, u_midpoint, dudt, nepochs=2000, dx=dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate the learned flow and compare to exact solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to integrate the learned model and compare to integration of the discretized true system. First, we set up the vector field that denotes the right-hand side of the spatially discretized PDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = x.size\n",
    "e = np.ones(M)\n",
    "D1 = .5/dx*spdiags([e,-e,e,-e], np.array([-M+1,-1,1,M-1]), M, M).toarray() # Central difference matrix\n",
    "D2 = 1/dx**2*spdiags([e,e,-2*e,e,e], np.array([-M+1,-1,0,1,M-1]), M, M).toarray() # 2nd order central difference matrix\n",
    "\n",
    "def true_f(u):\n",
    "    dH = -.5*eta*u**2 - (gamma**2 * u @ D2)\n",
    "    SdH = D1 @ dH\n",
    "    return SdH\n",
    "\n",
    "def learned_hnn_f(u):\n",
    "    u = torch.tensor(u.reshape(1,1,-1), requires_grad=True, dtype=torch.float32)\n",
    "    H = model.hamiltonian(u)\n",
    "    dH = torch.autograd.grad(H.sum(), u, retain_graph=False, create_graph=False)[0]\n",
    "    SdH = torch.tensor(D1, dtype=torch.float32) @ dH.reshape(-1,1)\n",
    "    return SdH.flatten().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide on an initial state for the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1, k2 = 0.6, 0.8\n",
    "d1, d2 = 0.3, 0.8\n",
    "M = x.size\n",
    "P = (x[0,-1]-x[0,0])*M/(M-1)\n",
    "sech = lambda a: 1/np.cosh(a)\n",
    "u0 = 0\n",
    "u0 += (-6./-eta)*2 * k1**2 * sech(np.abs(k1 * ((x+P/2-P*d1) % P - P/2)))**2\n",
    "u0 += (-6./-eta)*2 * k2**2 * sech(np.abs(k2 * ((x+P/2-P*d2) % P - P/2)))**2\n",
    "u0 = np.concatenate([u0[M:], u0[:M]], axis=0)[0,:]\n",
    "\n",
    "# u0 = u_exact[0,:] # If testing with initial condition from training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrate with the classic Rungeâ€“Kutta method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rk4(f, u, t_end, dt):\n",
    "    t_steps = np.arange(0, t_end + dt, dt)\n",
    "    u_steps = np.zeros((len(t_steps),) + np.shape(u))\n",
    "    \n",
    "    u_steps[0] = u\n",
    "    for i in range(1, len(t_steps)):\n",
    "        k1 = dt * f(u)\n",
    "        k2 = dt * f(u + .5*k1)\n",
    "        k3 = dt * f(u + .5*k2)\n",
    "        k4 = dt * f(u + k3)\n",
    "        u = u + (k1 + 2*k2 + 2*k3 + k4) / 6\n",
    "        u_steps[i] = u\n",
    "        \n",
    "    return t_steps, u_steps\n",
    "\n",
    "ts, u_true = rk4(true_f, u0, 4, .001)\n",
    "ts, u_learned = rk4(learned_hnn_f, u0, 4, .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (10, 3)\n",
    "indices = [\n",
    "    int(round((u_learned.shape[0] - 1) * 1/4)),\n",
    "    int(round((u_learned.shape[0] - 1) * 1/2)),\n",
    "    int(round((u_learned.shape[0] - 1) * 3/4)),\n",
    "    int(round((u_learned.shape[0] - 1)))\n",
    "]\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.plot(x[0, :], u_true[idx, :] if i < 3 else u_true[idx - 1, :], 'k-', label='Integrated true flow')\n",
    "    plt.plot(x[0, :], u_learned[idx, :], 'g-', label='Integrated learned flow')\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$u(t,x)$')\n",
    "    plt.title('$t = %.2f$' % ts[idx])\n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [phlearn](https://github.com/SINTEF/pseudo-hamiltonian-neural-networks) package can be used to set up PHNNs that can learn PDEs with damping (e.g. due to viscosity) and with external forces acting on the system. [This notebook](https://github.com/SINTEF/pseudo-hamiltonian-neural-networks/blob/main/example_scripts/kdv_example.ipynb) shows how to train a PHNN model for the forced KdV-Burgers equation, i.e.\n",
    "$$\n",
    "u_t + \\eta u u_x - \\nu u_{xx} + \\gamma^2 u_{xxx} = f(x, t).\n",
    "$$\n",
    "[This notebook](https://github.com/SINTEF/pseudo-hamiltonian-neural-networks/blob/main/example_scripts/phnn_pde_examples.ipynb) is set up to learn several different pseudo-Hamiltonian PDEs. In these notebooks you can also compare the PHNN models to baseline models that do not assume a pseudo-Hamiltonian structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nldl_env)",
   "language": "python",
   "name": "nldl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
