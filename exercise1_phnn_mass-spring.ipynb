{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHNN for a damped mass-spring system\n",
    "\n",
    "We train a [Hamiltonian neural network (HNN)](https://proceedings.neurips.cc/paper/2019/hash/26cd8ecadce0d4efd6cc8a8725cbd1f8-Abstract.html) (or [pseudo-Hamiltonian neural network (PHNN)](https://doi.org/10.1016/j.physd.2023.133673)) model to learn a (damped) mass-spring system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises:\n",
    "* Change `TRAJ_TRAIN` to experiment with training on one or several trajectory, and add noise to the data\n",
    "* Set up the neural network for the standard model and the PHNN by specifying the input and output dimensions\n",
    "* Make the damping coefficient trainable in the HNN, so that it becomes a PHNN\n",
    "* Change the damping coefficient `MU` to train on systems with and without damping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from tqdm import trange\n",
    "    import seaborn as sns\n",
    "except ModuleNotFoundError:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from tqdm import trange\n",
    "    import seaborn as sns\n",
    "if int(np.__version__.split('.')[0]) >= 2:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    print(\"NumPy version >= 2 detected. Downgrading to a compatible version...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy<2\"])\n",
    "    print(\"Please restart the kernel and rerun the script.\")\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.random.manual_seed(1)\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['lines.markersize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "colors = sns.color_palette([(0.6,0.8,.8), (1,0.7,0.3), (0.2,0.7,0.2), (0.8,0,0.2), (0,0.4,1), (0.6,0.5,.9), (0.5,0.3,.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact solution and data\n",
    "\n",
    "Here, we will learn the dynamics of a 1D damped mass-spring system with unit mass, spring constant $k$ and damping coefficient $\\mu$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dot{q} &= p, \\\\\n",
    "\\dot{p} &= -\\mu p - k q.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The system can be written as a pseudo-Hamiltonian system\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\begin{bmatrix}\n",
    " \\dot{q}\\\\\n",
    " \\dot{p}\n",
    "\\end{bmatrix} = \\left(\n",
    "\\begin{bmatrix}\n",
    "0 & 1\\\\\n",
    "-1 & 0\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0 & 0\\\\\n",
    "0 & -\\mu\n",
    "\\end{bmatrix}\n",
    "\\right) \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\mathcal{H}}{\\partial q}(q,p)\\\\\n",
    "\\frac{\\partial \\mathcal{H}}{\\partial p}(q,p)\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "with the Hamiltonian\n",
    "$$\n",
    "H(q, p) = \\frac{1}{2}p^2 + \\frac{k}{2}q^2.\n",
    "$$\n",
    "\n",
    "\n",
    "For the underdampened case, $\\mu^2 < 4 k~,$ the exact solution is given by \n",
    "$$\n",
    "\\begin{align*}\n",
    "q(t) &= e^{-\\delta t}(A \\cos(\\phi + \\omega t)),\\\\\n",
    "p(t) &= -A e^{-\\delta t}(\\omega \\sin(\\phi + \\omega t)) + \\delta \\cos(\\phi + \\omega t)\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\omega=\\frac{1}{2}\\sqrt{4k - \\mu^2}$ and the coefficients $A$ and $\\phi$ determine the initial conditions. We will set $A=1$ and $\\phi=0$ for the test data.\n",
    "\n",
    "Let us turn the above exact solution into a function that we can use to generate a training data set $X_{\\mathrm{train}}$ consisting of points $(t_i, x^j_i)$, where $t_i$ are evenly spaced times on some interval and $x^j_i=x^j(t_i)$ is given by the above exact solution for the initial state $x_0^j = x^j(t_0)$. We wil also create a test data set $X_{\\mathrm{test}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Experiment with different values for the damping coefficient, including $\\mu=0$, in which case the system is Hamiltonian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ODE parameters:\n",
    "MU = 0. # damping coefficient\n",
    "K = 1 # spring constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Experiment with different number of training trajectories, number of data points and the end time of the training trajectories. These depend on the goal of the modelling: do you want to extrapolate only in time, or also interpolate in space. Also experiment with adding noise to your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data generation parameters\n",
    "TRAJ_TRAIN = 5 # Number of distinct trajectories to train on\n",
    "N_TRAIN = 5 # Number of data points in each training trajectory\n",
    "TMAX_TRAIN = 1 # End time of each training trajectory\n",
    "NOISE_STD = 0. # Standard deviation of the noise added to the data\n",
    "\n",
    "TRAJ_TEST = 1 # Number of distinct trajectories to test on\n",
    "N_TEST = 4*64 # Number of data points in each test trajectory\n",
    "TMAX_TEST = 4 * (2*np.pi*np.sqrt(1/K)) # End time of each test trajectory\n",
    "\n",
    "if TRAJ_TRAIN == 1:\n",
    "    time_shift_max = 0\n",
    "else:\n",
    "    time_shift_max = 2*np.pi\n",
    "\n",
    "def exact_solution(t, A=1, phi=0, k=K, mu=MU):\n",
    "    \"\"\"Get exact solution to the 1D underdamped harmonic oscillator.\"\"\"\n",
    "    assert mu**2 < 4 * k, \"System must be underdamped.\"\n",
    "    w = np.sqrt(4 * k - mu**2) / 2\n",
    "    q = torch.exp(-mu / 2 * t) * A * torch.cos(phi + w * t)\n",
    "    p = -A * torch.exp(-mu / 2 * t) * (w * torch.sin(phi + w * t) + mu / 2 * torch.cos(phi + w * t))\n",
    "    x = torch.concat((q,p), axis = 1)\n",
    "    return x\n",
    "\n",
    "t_train = torch.linspace(0, TMAX_TRAIN, N_TRAIN + 1).unsqueeze(-1)\n",
    "time_shifts = time_shift_max*torch.rand(1, TRAJ_TRAIN)\n",
    "t_train_shifted = t_train.expand(-1, TRAJ_TRAIN).unsqueeze(1) + time_shifts.unsqueeze(0).repeat((t_train.shape[0],1,1))\n",
    "\n",
    "t_test = torch.linspace(0, TMAX_TEST, N_TEST + 1).unsqueeze(-1)\n",
    "time_shifts = time_shift_max*torch.rand(1, TRAJ_TEST)\n",
    "t_test_shifted = t_test.expand(-1, TRAJ_TEST).unsqueeze(1) + time_shifts.unsqueeze(0).repeat((t_test.shape[0],1,1))\n",
    "\n",
    "x_train = []\n",
    "for i in range(TRAJ_TRAIN):\n",
    "    if TRAJ_TRAIN == 1: # If we only have one training trajectory, we want to train and test on the same trajectory\n",
    "        x_train.append(exact_solution(t_train_shifted[:,:,i]))\n",
    "    else:\n",
    "        A = torch.rand(1).item()+.3\n",
    "        phi = torch.rand(1).item()-.5\n",
    "        x_train.append(exact_solution(t_train_shifted[:,:,i], A, phi))\n",
    "x_train = torch.stack(x_train, axis=2)\n",
    "x_train = x_train + NOISE_STD * torch.randn_like(x_train)\n",
    "x_test = exact_solution(t_test_shifted)\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(t_test, x_test[:, 0], color = 'k', linestyle=\"-\", label=\"Test data (exact sol.)\")\n",
    "for i in range(min(TRAJ_TRAIN,len(colors))):\n",
    "    plt.plot(t_train, x_train[:, 0, i], color = colors[i], linestyle=\"none\", marker=\".\", label=f'Training trajectory {i+1}')\n",
    "plt.title(\"Data\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$q$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(x_test[:, 0], x_test[:, 1], color = 'k', linestyle=\"-\", label=\"Test data (exact sol.)\")\n",
    "for i in range(min(TRAJ_TRAIN,len(colors))):\n",
    "    plt.plot(x_train[:, 0, i], x_train[:, 1, i], color = colors[i], linestyle=\"none\", marker=\".\", label=f'Training trajectory {i+1}')\n",
    "plt.xlabel(\"$q$\")\n",
    "plt.ylabel(\"$p$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a standard feedforward neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional exercise:** Experiment with different neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=1, hidden_size=32, output_size=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fnn = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fnn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the training process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not aiming to predict states of $q$ and $p$ directly, but rather learn a model $\\hat{g}_\\theta$ for the vector field $g : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$ so that the first-order ordinary differential equation (ODE)\n",
    "$$\n",
    "\\dot{x} = g(x),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "x = \\begin{bmatrix}\n",
    "q \\\\\n",
    "p\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "describes the system from which the data is obtained. Thus, we want to have a loss function\n",
    "$$\n",
    "\\mathcal{L} = \\lVert \\dot{x} - \\hat{g}_\\theta(x) \\rVert.\n",
    "$$\n",
    "\n",
    "Unfortunately, we do not have the time-derivates of $q$ and $p$ (we do not assume to know that $p = \\dot{q}$), so these must be approximated. By approximating them with finite difference and evaluating $g$ on the midpoint between each data point, we effectively train on the implicit midpoint integration scheme\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\frac{x_{i+1} - x_{i}}{\\Delta t} = g \\left( \\frac{x_{i}+x_{i+1}}{2} \\right),\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $\\Delta t = t_{n+1} - t_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_midpoint = ((x_train + x_train.roll(shifts=1, dims=0))[1:,:,:]/2).requires_grad_()\n",
    "dxdt = (x_train - x_train.roll(shifts=1, dims=0))[1:,:,:]/(t_train - t_train.roll(shifts=1, dims=0))[1:,:].reshape(-1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking the data for training:\n",
    "x_midpoint_splits = torch.split(x_midpoint, 1, dim=2)\n",
    "x_midpoint_reshaped = torch.stack(x_midpoint_splits, dim=0).squeeze(2).reshape(-1, 2).requires_grad_()\n",
    "dxdt_splits = torch.split(dxdt, 1, dim=2)\n",
    "dxdt_reshaped = torch.stack(dxdt_splits, dim=0).squeeze(2).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, dxdt, nepochs=10000, learning_rate=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses = []\n",
    "    \n",
    "    with trange(nepochs, desc=\"Training the model\") as pbar:\n",
    "        for i in range(nepochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute the loss from the difference between the left-hand side and right-hand side of the ODE:\n",
    "            ode_rhs = model(x)\n",
    "            loss = torch.mean((dxdt - ode_rhs) ** 2)\n",
    "            \n",
    "            # Backpropagation and optimization step:\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Log the loss value:\n",
    "            losses.append(loss.item())\n",
    "            if i % 100 == 0 or i == nepochs - 1:\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Plot the loss curve\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(losses)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a purely data-driven model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we will first train a model that does not assume that the system has a Hamiltonian structure. We assume then only that the system can be formulated as a first-order ODE $\\dot{x} = g(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Set the input and output dimensions of the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_snn = FNN(SET_INPUT_DIM, 32, SET_OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model_snn, x_midpoint_reshaped, dxdt_reshaped, nepochs=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an PHNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Set the input and output dimensions of the PHNN.\n",
    "\n",
    "**Exercise:** Experiment with making the damping coefficient not learnable, so that the model becomes a pure Hamiltonian neural network (HNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PHNN(nn.Module):\n",
    "    def __init__(self, input_dim=SET_INPUT_DIM, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.hamiltonian_net = FNN(input_size=input_dim, hidden_size=hidden_dim, output_size=SET_OUTPUT_DIM)\n",
    "        self.mu = nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
    "        self.S = torch.tensor([[0.0, 1.0], [-1.0, 0.0]])\n",
    "\n",
    "    def hamiltonian(self, x):\n",
    "        return self.hamiltonian_net(x)\n",
    "\n",
    "    def grad(self, x):\n",
    "        return torch.autograd.grad(self.hamiltonian(x).sum(), x, create_graph=True)[0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        S = self.S.clone().detach()\n",
    "        S[1, 1] = -self.mu\n",
    "        dH = self.grad(x)\n",
    "        return (S @ dH.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_phnn = PHNN()\n",
    "train(model_phnn, x_midpoint_reshaped, dxdt_reshaped, nepochs=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Learned damping coefficient: {model_phnn.mu.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating the learned models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have learned two different models for the vector field $g$ that describes the dynamics of the systems by the ODE $\\dot{x} = g{x}$. To obtain the predictions of $x$ we need to integrate this model from an inital state $x_0 = x(t_0)$. Let us use the Rungeâ€“Kutta method for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rk4(g, x0, t_end, dt):\n",
    "    x = x0\n",
    "    num_steps = int(t_end / dt)\n",
    "    solution = torch.zeros(num_steps+1, *x0.shape)\n",
    "    solution[0] = x0\n",
    "    \n",
    "    for i in range(1, num_steps+1):\n",
    "        k1 = dt * g(x)\n",
    "        k2 = dt * g(x + 0.5 * k1)\n",
    "        k3 = dt * g(x + 0.5 * k2)\n",
    "        k4 = dt * g(x + k3)\n",
    "        \n",
    "        x = x + (1/6) * (k1 + 2*k2 + 2*k3 + k4)\n",
    "        solution[i] = x\n",
    "    \n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = x_test[0,:,0].requires_grad_()\n",
    "\n",
    "x_test_snn = rk4(model_snn, x0, t_test[-1], TMAX_TEST/N_TEST).detach().numpy()\n",
    "x_test_phnn = rk4(model_phnn, x0, t_test[-1], TMAX_TEST/N_TEST).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(x_test[:, 0], x_test[:, 1], \"k-\", label=\"Exact solution\")\n",
    "plt.plot(x_test_snn[:, 0], x_test_snn[:, 1], \"r-\", label=\"Standard NN\")\n",
    "plt.plot(x_test_phnn[:, 0], x_test_phnn[:, 1], \"g-\", label=\"PHNN\")\n",
    "buffer_x = 0.1 * (x_test[:, 0].max() - x_test[:, 0].min())\n",
    "buffer_y = 0.1 * (x_test[:, 1].max() - x_test[:, 1].min())\n",
    "plt.xlim(x_test[:, 0].min() - buffer_x, x_test[:, 0].max() + buffer_x)\n",
    "plt.ylim(x_test[:, 1].min() - buffer_y, x_test[:, 1].max() + buffer_y)\n",
    "plt.xlabel(\"$q$\")\n",
    "plt.ylabel(\"$p$\")\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(t_test, x_test[:, 0], 'k-', linewidth=2, label='Exact solution')\n",
    "plt.plot(t_test, x_test_snn[:, 0], 'r-', linewidth=2, label='Standard NN solution')\n",
    "plt.plot(t_test, x_test_phnn[:, 0], 'g-', linewidth=2, label='PHNN solution')\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$q$\")\n",
    "plt.legend(fontsize=12, loc='upper center', bbox_to_anchor=(0.5, -0.25), ncol=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean squared errors:\n",
    "mse_snn = torch.mean((x_test[:,:,0] - x_test_snn)**2, axis=1)\n",
    "mse_phnn = torch.mean((x_test[:,:,0] - x_test_phnn)**2, axis=1)\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.semilogy(t_test, mse_snn, \"r-\", label=\"Standard NN\")\n",
    "plt.semilogy(t_test, mse_phnn, \"g-\", label=\"PHNN\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Hamiltonians:\n",
    "H_exact = 0.5 * (x_test[:, 1].numpy()**2 + K * x_test[:, 0].numpy()**2)\n",
    "H_phnn = 0.5 * (x_test_phnn[:, 1]**2 + K * x_test_phnn[:, 0]**2)\n",
    "modelH_exact = model_phnn.hamiltonian(x_test[:,:,0]).detach().numpy()\n",
    "modelH_phnn = model_phnn.hamiltonian(torch.from_numpy(x_test_phnn).float()).detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(t_test, H_exact, 'k-', linewidth=2, label='Exact Hamiltonian on true data')\n",
    "plt.plot(t_test, H_phnn, 'g-', linewidth=2, label='Exact Hamiltonian on PHNN prediction')\n",
    "plt.plot(t_test, modelH_exact, 'k--', linewidth=2, label='Learned Hamiltonian on true data')\n",
    "plt.plot(t_test, modelH_phnn, 'g--', linewidth=2, label='Learned Hamiltonian on PHNN prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Hamiltonian')\n",
    "plt.legend(fontsize=12, loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [phlearn](https://github.com/SINTEF/pseudo-hamiltonian-neural-networks) package can be used to set up PHNNs that can learn systems of arbitrary dimensions and with external forces. As a first step, [this notebook](https://github.com/SINTEF/pseudo-hamiltonian-neural-networks/blob/main/example_scripts/spring_example.ipynb) shows how to train a PHNN model for a damped mass-spring system with external forces acting on it. That is,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{d q}{d t} &= p, \\\\\n",
    "\\frac{d p}{d t} &= -\\mu p - k q + f(q, p, t).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nldl_env)",
   "language": "python",
   "name": "nldl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
